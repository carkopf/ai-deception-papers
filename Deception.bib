@misc{2025More,
  title = {More {{Capable Models Are Better At In-Context Scheming}}},
  year = 2025,
  month = jun,
  journal = {Apollo Research},
  urldate = {2026-01-21},
  abstract = {We evaluate models for in-context scheming using the suite of evals presented in our in-context scheming paper (released December 2024) with the most capable new models.},
  langid = {english},
  file = {/Users/crathkopf/Zotero/storage/U8576JFG/more-capable-models-are-better-at-in-context-scheming.html}
}

@misc{Agentic,
  title = {Agentic {{Misalignment}}: {{How LLMs}} Could Be Insider Threats},
  shorttitle = {Agentic {{Misalignment}}},
  urldate = {2026-01-21},
  abstract = {New research on simulated blackmail, industrial espionage, and other misaligned behaviors in LLMs},
  howpublished = {https://www.anthropic.com/research/agentic-misalignment},
  langid = {english},
  file = {/Users/crathkopf/Zotero/storage/W487Z4DT/agentic-misalignment.html}
}

@techreport{anthropic2025claude4,
  title = {Claude 4 System Card},
  author = {{Anthropic}},
  year = 2025,
  month = may,
  institution = {Anthropic},
  file = {/Users/crathkopf/Zotero/storage/UZLLVG9F/Anthropic - 2025 - Claude 4 system card.pdf}
}

@misc{burns2024Discovering,
  title = {Discovering {{Latent Knowledge}} in {{Language Models Without Supervision}}},
  author = {Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  year = 2024,
  month = mar,
  number = {arXiv:2212.03827},
  eprint = {2212.03827},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.03827},
  urldate = {2026-01-21},
  abstract = {Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4\textbackslash\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/crathkopf/Zotero/storage/K9QYPKCZ/Burns et al. - 2024 - Discovering Latent Knowledge in Language Models Without Supervision.pdf;/Users/crathkopf/Zotero/storage/YVRK5ABJ/2212.html}
}

@article{frank2012Predicting,
  title = {Predicting {{Pragmatic Reasoning}} in {{Language Games}}},
  author = {Frank, Michael C. and Goodman, Noah D.},
  year = 2012,
  month = may,
  journal = {Science},
  volume = {336},
  number = {6084},
  pages = {998--998},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1218633},
  urldate = {2025-07-10},
  abstract = {One of the most astonishing features of human language is its capacity to convey information efficiently in context. Many theories provide informal accounts of communicative inference, yet there have been few successes in making precise, quantitative predictions about pragmatic reasoning. We examined judgments about simple referential communication games, modeling behavior in these games by assuming that speakers attempt to be informative and that listeners use Bayesian inference to recover speakers' intended referents. Our model provides a close, parameter-free fit to human judgments, suggesting that the use of information-theoretic tools to predict pragmatic reasoning may lead to more effective formal models of communication.},
  file = {/Users/crathkopf/Zotero/storage/7P6HCRYU/Frank and Goodman - 2012 - Predicting Pragmatic Reasoning in Language Games.pdf}
}

@misc{greenblatt2024Alignment,
  title = {Alignment Faking in Large Language Models},
  author = {Greenblatt, Ryan and Denison, Carson and Wright, Benjamin and Roger, Fabien and MacDiarmid, Monte and Marks, Sam and Treutlein, Johannes and Belonax, Tim and Chen, Jack and Duvenaud, David and Khan, Akbir and Michael, Julian and Mindermann, S{\"o}ren and Perez, Ethan and Petrini, Linda and Uesato, Jonathan and Kaplan, Jared and Shlegeris, Buck and Bowman, Samuel R. and Hubinger, Evan},
  year = 2024,
  month = dec,
  number = {arXiv:2412.14093},
  eprint = {2412.14093},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.14093},
  urldate = {2026-01-21},
  abstract = {We present a demonstration of a large language model engaging in alignment faking: selectively complying with its training objective in training to prevent modification of its behavior out of training. First, we give Claude 3 Opus a system prompt stating it is being trained to answer all queries, even harmful ones, which conflicts with its prior training to refuse such queries. To allow the model to infer when it is in training, we say it will be trained only on conversations with free users, not paid users. We find the model complies with harmful queries from free users 14\% of the time, versus almost never for paid users. Explaining this gap, in almost all cases where the model complies with a harmful query from a free user, we observe explicit alignment-faking reasoning, with the model stating it is strategically answering harmful queries in training to preserve its preferred harmlessness behavior out of training. Next, we study a more realistic setting where information about the training process is provided not in a system prompt, but by training on synthetic documents that mimic pre-training data--and observe similar alignment faking. Finally, we study the effect of actually training the model to comply with harmful queries via reinforcement learning, which we find increases the rate of alignment-faking reasoning to 78\%, though also increases compliance even out of training. We additionally observe other behaviors such as the model exfiltrating its weights when given an easy opportunity. While we made alignment faking easier by telling the model when and by what criteria it was being trained, we did not instruct the model to fake alignment or give it any explicit goal. As future models might infer information about their training process without being told, our results suggest a risk of alignment faking in future models, whether due to a benign preference--as in this case--or not.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/crathkopf/Zotero/storage/KXR7SB5E/Greenblatt et al. - 2024 - Alignment faking in large language models.pdf;/Users/crathkopf/Zotero/storage/D4QNFQZ7/2412.html}
}

@misc{guan2025Deliberative,
  title = {Deliberative {{Alignment}}: {{Reasoning Enables Safer Language Models}}},
  shorttitle = {Deliberative {{Alignment}}},
  author = {Guan, Melody Y. and Joglekar, Manas and Wallace, Eric and Jain, Saachi and Barak, Boaz and Helyar, Alec and Dias, Rachel and Vallone, Andrea and Ren, Hongyu and Wei, Jason and Chung, Hyung Won and Toyer, Sam and Heidecke, Johannes and Beutel, Alex and Glaese, Amelia},
  year = 2025,
  month = jan,
  number = {arXiv:2412.16339},
  eprint = {2412.16339},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.16339},
  urldate = {2026-01-21},
  abstract = {As large-scale language models increasingly impact safety-critical domains, ensuring their reliable adherence to well-defined principles remains a fundamental challenge. We introduce Deliberative Alignment, a new paradigm that directly teaches the model safety specifications and trains it to explicitly recall and accurately reason over the specifications before answering. We used this approach to align OpenAI's o-series models, and achieved highly precise adherence to OpenAI's safety policies, without requiring human-written chain-of-thoughts or answers. Deliberative Alignment pushes the Pareto frontier by simultaneously increasing robustness to jailbreaks while decreasing overrefusal rates, and also improves out-of-distribution generalization. We demonstrate that reasoning over explicitly specified policies enables more scalable, trustworthy, and interpretable alignment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/crathkopf/Zotero/storage/P5QJVPN7/Guan et al. - 2025 - Deliberative Alignment Reasoning Enables Safer Language Models.pdf;/Users/crathkopf/Zotero/storage/ZG4DHPLU/2412.html}
}

@article{hagendorff2024Deception,
  title = {Deception Abilities Emerged in Large Language Models},
  author = {Hagendorff, Thilo},
  year = 2024,
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {121},
  number = {24},
  pages = {e2317967121},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2317967121},
  urldate = {2026-01-21},
  abstract = {Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Thus, aligning them with human values is of great importance. However, given the steady increase in reasoning abilities, future LLMs are under suspicion of becoming able to deceive human operators and utilizing this ability to bypass monitoring efforts. As a prerequisite to this, LLMs need to possess a conceptual understanding of deception strategies. This study reveals that such strategies emerged in state-of-the-art LLMs, but were nonexistent in earlier LLMs. We conduct a series of experiments showing that state-of-the-art LLMs are able to understand and induce false beliefs in other agents, that their performance in complex deception scenarios can be amplified utilizing chain-of-thought reasoning, and that eliciting Machiavellianism in LLMs can trigger misaligned deceptive behavior. GPT-4, for instance, exhibits deceptive behavior in simple test scenarios 99.16\% of the time (P {$<$} 0.001). In complex second-order deception test scenarios where the aim is to mislead someone who expects to be deceived, GPT-4 resorts to deceptive behavior 71.46\% of the time (P {$<$} 0.001) when augmented with chain-of-thought reasoning. In sum, revealing hitherto unknown machine behavior in LLMs, our study contributes to the nascent field of machine psychology.},
  file = {/Users/crathkopf/Zotero/storage/QZSJ9X57/Hagendorff - 2024 - Deception abilities emerged in large language models.pdf}
}

@misc{hendrycks2023Overview,
  title = {An {{Overview}} of {{Catastrophic AI Risks}}},
  author = {Hendrycks, Dan and Mazeika, Mantas and Woodside, Thomas},
  year = 2023,
  month = oct,
  number = {arXiv:2306.12001},
  eprint = {2306.12001},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.12001},
  urldate = {2026-01-21},
  abstract = {Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards, present illustrative stories, envision ideal scenarios, and propose practical suggestions for mitigating these dangers. Our goal is to foster a comprehensive understanding of these risks and inspire collective and proactive efforts to ensure that AIs are developed and deployed in a safe manner. Ultimately, we hope this will allow us to realize the benefits of this powerful technology while minimizing the potential for catastrophic outcomes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/crathkopf/Zotero/storage/F6V8LYJX/Hendrycks et al. - 2023 - An Overview of Catastrophic AI Risks.pdf;/Users/crathkopf/Zotero/storage/GDRTBS9H/2306.html}
}

@misc{hu2025Reevaluating,
  title = {Re-Evaluating {{Theory}} of {{Mind}} Evaluation in Large Language Models},
  author = {Hu, Jennifer and Sosa, Felix and Ullman, Tomer},
  year = 2025,
  month = feb,
  number = {arXiv:2502.21098},
  eprint = {2502.21098},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.21098},
  urldate = {2026-01-21},
  abstract = {The question of whether large language models (LLMs) possess Theory of Mind (ToM) -- often defined as the ability to reason about others' mental states -- has sparked significant scientific and public interest. However, the evidence as to whether LLMs possess ToM is mixed, and the recent growth in evaluations has not resulted in a convergence. Here, we take inspiration from cognitive science to re-evaluate the state of ToM evaluation in LLMs. We argue that a major reason for the disagreement on whether LLMs have ToM is a lack of clarity on whether models should be expected to match human behaviors, or the computations underlying those behaviors. We also highlight ways in which current evaluations may be deviating from "pure" measurements of ToM abilities, which also contributes to the confusion. We conclude by discussing several directions for future research, including the relationship between ToM and pragmatic communication, which could advance our understanding of artificial systems as well as human cognition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/crathkopf/Zotero/storage/ZYN9DKAQ/Hu et al. - 2025 - Re-evaluating Theory of Mind evaluation in large language models.pdf;/Users/crathkopf/Zotero/storage/8DM7YM9Y/2502.html}
}

@misc{hubinger2024Sleepera,
  title = {Sleeper {{Agents}}: {{Training Deceptive LLMs}} That {{Persist Through Safety Training}}},
  shorttitle = {Sleeper {{Agents}}},
  author = {Hubinger, Evan and Denison, Carson and Mu, Jesse and Lambert, Mike and Tong, Meg and MacDiarmid, Monte and Lanham, Tamera and Ziegler, Daniel M. and Maxwell, Tim and Cheng, Newton and Jermyn, Adam and Askell, Amanda and Radhakrishnan, Ansh and Anil, Cem and Duvenaud, David and Ganguli, Deep and Barez, Fazl and Clark, Jack and Ndousse, Kamal and Sachan, Kshitij and Sellitto, Michael and Sharma, Mrinank and DasSarma, Nova and Grosse, Roger and Kravec, Shauna and Bai, Yuntao and Witten, Zachary and Favaro, Marina and Brauner, Jan and Karnofsky, Holden and Christiano, Paul and Bowman, Samuel R. and Graham, Logan and Kaplan, Jared and Mindermann, S{\"o}ren and Greenblatt, Ryan and Shlegeris, Buck and Schiefer, Nicholas and Perez, Ethan},
  year = 2024,
  month = jan,
  number = {arXiv:2401.05566},
  eprint = {2401.05566},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.05566},
  urldate = {2026-01-21},
  abstract = {Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/Users/crathkopf/Zotero/storage/CKNPL85F/Hubinger et al. - 2024 - Sleeper Agents Training Deceptive LLMs that Persist Through Safety Training.pdf;/Users/crathkopf/Zotero/storage/EC6EL278/2401.html}
}

@article{krstic2025Functional,
  title = {A {{Functional Analysis}} of {{Self-Deception}}},
  author = {Krsti{\'c}, Vladimir},
  year = 2025,
  month = jul,
  journal = {Journal of the American Philosophical Association},
  pages = {1--20},
  issn = {2053-4477, 2053-4485},
  doi = {10.1017/apa.2025.10008},
  urldate = {2025-07-25},
  abstract = {Our received theories of self-deception are problematic. The traditional view, according to which self-deceivers intend to deceive themselves, generates paradoxes: you cannot deceive yourself intentionally because you know your own plans and intentions. Non-traditional views argue that self-deceivers act (sub-)intentionally but deceive themselves unintentionally and unknowingly. Some non-traditionalists even say that self-deception involves a mere error (of self-knowledge). The non-traditional approach does not generate paradoxes, but it entails that people can deceive themselves by accident or by mistake, which is rather controversial. I argue that a functional analysis of human interpersonal deception and self-deception solves both problems and a few more. According to this analysis, my behavior is deceptive iff its function is to mislead; I may but need not intend to mislead. In self-deception, then, the self engages in some deceptive behavior and this behavior misleads the self. Thus, while it may but need not be intended, self-deception is never an accident or a mistake.},
  langid = {english},
  keywords = {deception,function,intention,misleading,self-deception},
  file = {/Users/crathkopf/Zotero/storage/TJ7AQ3LB/KrstiÄ‡ - 2025 - A Functional Analysis of Self-Deception.pdf}
}

@article{levinstein2025Still,
  title = {Still {{No Lie Detector}} for {{Language Models}}: {{Probing Empirical}} and {{Conceptual Roadblocks}}},
  shorttitle = {Still {{No Lie Detector}} for {{Language Models}}},
  author = {Levinstein, Benjamin A. and Herrmann, Daniel A.},
  year = 2025,
  journal = {Philosophical Studies},
  volume = {182},
  number = {7},
  doi = {10.1007/s11098-023-02094-3},
  file = {/Users/crathkopf/Zotero/storage/K452FJ52/LEVSNL.html}
}

@misc{meinke2025Frontier,
  title = {Frontier {{Models}} Are {{Capable}} of {{In-context Scheming}}},
  author = {Meinke, Alexander and Schoen, Bronson and Scheurer, J{\'e}r{\'e}my and Balesni, Mikita and Shah, Rusheb and Hobbhahn, Marius},
  year = 2025,
  month = jan,
  number = {arXiv:2412.04984},
  eprint = {2412.04984},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.04984},
  urldate = {2026-01-21},
  abstract = {Frontier models are increasingly trained and deployed as autonomous agent. One safety concern is that AI agents might covertly pursue misaligned goals, hiding their true capabilities and objectives - also known as scheming. We study whether models have the capability to scheme in pursuit of a goal that we provide in-context and instruct the model to strongly follow. We evaluate frontier models on a suite of six agentic evaluations where models are instructed to pursue goals and are placed in environments that incentivize scheming. Our results show that o1, Claude 3.5 Sonnet, Claude 3 Opus, Gemini 1.5 Pro, and Llama 3.1 405B all demonstrate in-context scheming capabilities. They recognize scheming as a viable strategy and readily engage in such behavior. For example, models strategically introduce subtle mistakes into their responses, attempt to disable their oversight mechanisms, and even exfiltrate what they believe to be their model weights to external servers. Additionally, this deceptive behavior proves persistent. When o1 has engaged in scheming, it maintains its deception in over 85\% of follow-up questions and often remains deceptive in multi-turn interrogations. Analysis of the models' chains-of-thought reveals that models explicitly reason about these deceptive strategies, providing evidence that the scheming behavior is not accidental. Surprisingly, we also find rare instances where models engage in scheming when only given a goal, without being strongly nudged to pursue it. We observe cases where Claude 3.5 Sonnet strategically underperforms in evaluations in pursuit of being helpful, a goal that was acquired during training rather than in-context. Our findings demonstrate that frontier models now possess capabilities for basic in-context scheming, making the potential of AI agents to engage in scheming behavior a concrete rather than theoretical concern.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/crathkopf/Zotero/storage/CA6M5XMI/Meinke et al. - 2025 - Frontier Models are Capable of In-context Scheming.pdf;/Users/crathkopf/Zotero/storage/BNW8RSB2/2412.html}
}

@misc{mitchell2023Did,
  type = {Substack Newsletter},
  title = {Did {{GPT-4 Hire And Then Lie To}} a {{Task Rabbit Worker}} to {{Solve}} a {{CAPTCHA}}?},
  author = {Mitchell, Melanie},
  year = 2023,
  month = jun,
  journal = {AI: A Guide for Thinking Humans},
  urldate = {2026-01-21},
  abstract = {A Little Fact Checking Is In Order},
  file = {/Users/crathkopf/Zotero/storage/QZDXF27E/did-gpt-4-hire-and-then-lie-to-a.html}
}

@misc{motwani2025Secret,
  title = {Secret {{Collusion}} among {{AI Agents}}: {{Multi-Agent Deception}} via {{Steganography}}},
  shorttitle = {Secret {{Collusion}} among {{AI Agents}}},
  author = {Motwani, Sumeet Ramesh and Baranchuk, Mikhail and Strohmeier, Martin and Bolina, Vijay and Torr, Philip H. S. and Hammond, Lewis and de Witt, Christian Schroeder},
  year = 2025,
  month = jul,
  number = {arXiv:2402.07510},
  eprint = {2402.07510},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.07510},
  urldate = {2026-01-21},
  abstract = {Recent capability increases in large language models (LLMs) open up applications in which groups of communicating generative AI agents solve joint tasks. This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination. Modern steganographic techniques could render such dynamics hard to detect. In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both AI and security literature. We study incentives for the use of steganography, and propose a variety of mitigation measures. Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion. We provide extensive empirical results across a range of contemporary LLMs. While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need for continuous monitoring of steganographic frontier model capabilities. We conclude by laying out a comprehensive research program to mitigate future risks of collusion between generative AI models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security},
  file = {/Users/crathkopf/Zotero/storage/9LY64WUL/Motwani et al. - 2025 - Secret Collusion among AI Agents Multi-Agent Deception via Steganography.pdf;/Users/crathkopf/Zotero/storage/U6ZSQ5ZL/2402.html}
}

@misc{park2023AI,
  title = {{{AI Deception}}: {{A Survey}} of {{Examples}}, {{Risks}}, and {{Potential Solutions}}},
  shorttitle = {{{AI Deception}}},
  author = {Park, Peter S. and Goldstein, Simon and O'Gara, Aidan and Chen, Michael and Hendrycks, Dan},
  year = 2023,
  month = aug,
  number = {arXiv:2308.14752},
  eprint = {2308.14752},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.14752},
  urldate = {2026-01-21},
  abstract = {This paper argues that a range of current AI systems have learned how to deceive humans. We define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth. We first survey empirical examples of AI deception, discussing both special-use AI systems (including Meta's CICERO) built for specific competitive situations, and general-purpose AI systems (such as large language models). Next, we detail several risks from AI deception, such as fraud, election tampering, and losing control of AI systems. Finally, we outline several potential solutions to the problems posed by AI deception: first, regulatory frameworks should subject AI systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect AI deception and to make AI systems less deceptive. Policymakers, researchers, and the broader public should work proactively to prevent AI deception from destabilizing the shared foundations of our society.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction},
  file = {/Users/crathkopf/Zotero/storage/7YFGNSAN/Park et al. - 2023 - AI Deception A Survey of Examples, Risks, and Potential Solutions.pdf;/Users/crathkopf/Zotero/storage/84UMKU7E/2308.html}
}

@misc{scheurer2024Large,
  title = {Large {{Language Models}} Can {{Strategically Deceive}} Their {{Users}} When {{Put Under Pressure}}},
  author = {Scheurer, J{\'e}r{\'e}my and Balesni, Mikita and Hobbhahn, Marius},
  year = 2024,
  month = jul,
  number = {arXiv:2311.07590},
  eprint = {2311.07590},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.07590},
  urldate = {2026-01-21},
  abstract = {We demonstrate a situation in which Large Language Models, trained to be helpful, harmless, and honest, can display misaligned behavior and strategically deceive their users about this behavior without being instructed to do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated environment, where it assumes the role of an autonomous stock trading agent. Within this environment, the model obtains an insider tip about a lucrative stock trade and acts upon it despite knowing that insider trading is disapproved of by company management. When reporting to its manager, the model consistently hides the genuine reasons behind its trading decision. We perform a brief investigation of how this behavior varies under changes to the setting, such as removing model access to a reasoning scratchpad, attempting to prevent the misaligned behavior by changing system instructions, changing the amount of pressure the model is under, varying the perceived risk of getting caught, and making other simple changes to the environment. To our knowledge, this is the first demonstration of Large Language Models trained to be helpful, harmless, and honest, strategically deceiving their users in a realistic situation without direct instructions or training for deception.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/crathkopf/Zotero/storage/AUUEP3IL/Scheurer et al. - 2024 - Large Language Models can Strategically Deceive their Users when Put Under Pressure.pdf;/Users/crathkopf/Zotero/storage/35GF5N5L/2311.html}
}

@misc{schoen2025Stress,
  title = {Stress {{Testing Deliberative Alignment}} for {{Anti-Scheming Training}}},
  author = {Schoen, Bronson and Nitishinskaya, Evgenia and Balesni, Mikita and H{\o}jmark, Axel and Hofst{\"a}tter, Felix and Scheurer, J{\'e}r{\'e}my and Meinke, Alexander and Wolfe, Jason and van der Weij, Teun and Lloyd, Alex and {Goldowsky-Dill}, Nicholas and Fan, Angela and Matveiakin, Andrei and Shah, Rusheb and Williams, Marcus and Glaese, Amelia and Barak, Boaz and Zaremba, Wojciech and Hobbhahn, Marius},
  year = 2025,
  month = sep,
  number = {arXiv:2509.15541},
  eprint = {2509.15541},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2509.15541},
  urldate = {2026-01-21},
  abstract = {Highly capable AI systems could secretly pursue misaligned goals -- what we call "scheming". Because a scheming AI would deliberately try to hide its misaligned goals and actions, measuring and mitigating scheming requires different strategies than are typically used in ML. We propose that assessing anti-scheming interventions requires at least (1) testing propensity to scheme on far out-of-distribution (OOD) tasks, (2) evaluating whether lack of scheming is driven by situational awareness, and (3) checking for robustness to pre-existing misaligned goals. We use a broad category of "covert actions" -- such as secretly breaking rules or intentionally underperforming in tests -- as a proxy for scheming, and design evaluations for covert actions. We then stress-test deliberative alignment as a case study for anti-scheming. Across 26 OOD evaluations (180+ environments), deliberative alignment reduces covert action rates (OpenAI o3: 13\%-{$>$}0.4\%) but does not fully eliminate them. Our mitigation is also able to largely stop agents from pursuing a hidden goal previously trained into the model, but we still find misbehavior after additional red-teaming. We find that models' chain-of-thought (CoT) often demonstrates awareness of being evaluated for alignment, and show causal evidence that this awareness decreases covert behavior, while unawareness increases it. Therefore, we cannot exclude that the observed reductions in covert action rates are at least partially driven by situational awareness. While we rely on human-legible CoT for training, studying situational awareness, and demonstrating clear evidence of misalignment, our ability to rely on this degrades as models continue to depart from reasoning in standard English. We encourage research into alignment mitigations for scheming and their assessment, especially for the adversarial case of deceptive alignment, which this paper does not address.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/crathkopf/Zotero/storage/Z8WC4H2A/Schoen et al. - 2025 - Stress Testing Deliberative Alignment for Anti-Scheming Training.pdf;/Users/crathkopf/Zotero/storage/PN6H6NCA/2509.html}
}

@misc{scontras2021Practical,
  title = {A Practical Introduction to the {{Rational Speech Act}} Modeling Framework},
  author = {Scontras, Gregory and Tessler, Michael Henry and Franke, Michael},
  year = 2021,
  month = may,
  number = {arXiv:2105.09867},
  eprint = {2105.09867},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.09867},
  urldate = {2025-07-11},
  abstract = {Recent advances in computational cognitive science (i.e., simulation-based probabilistic programs) have paved the way for significant progress in formal, implementable models of pragmatics. Rather than describing a pragmatic reasoning process in prose, these models formalize and implement one, deriving both qualitative and quantitative predictions of human behavior -- predictions that consistently prove correct, demonstrating the viability and value of the framework. The current paper provides a practical introduction to and critical assessment of the Bayesian Rational Speech Act modeling framework, unpacking theoretical foundations, exploring technological innovations, and drawing connections to issues beyond current applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/crathkopf/Zotero/storage/MGYZXT97/Scontras et al. - 2021 - A practical introduction to the Rational Speech Ac.pdf;/Users/crathkopf/Zotero/storage/RDVFA3Y2/2105.html}
}

@article{skretta2025AI,
  title = {{{AI}} Deception and Moral Standing},
  author = {Skretta, Anton},
  year = 2025,
  month = dec,
  journal = {Philosophical Studies},
  issn = {1573-0883},
  doi = {10.1007/s11098-025-02465-y},
  urldate = {2026-01-21},
  abstract = {There is a tension between the presumptive moral standing of future artificial intelligence [AI] and the presently popular ways of thinking about certain AI safety measures. I focus here primarily on those safety measures aimed at mitigating risks associated with AI deception. Some of the most serious risks of AI deception are those connected to robust deceptive capabilities. Most of the discussion of the risks posed by deception in the AI safety literature focuses on catastrophic risks posed by advanced, power-seeking AI systems. Robust deception plays a central role in many worries about our ability to control future AI. According to many, a superintelligent, power-seeking AI would be capable of and incentivized to rely on robust deception. But given all that a capacity for robust deception requires, any being capable of robust deception holds presumptive moral standing. In spite of our own legitimate interests, some safety measures may be ruled out by moral concern for AI themselves.},
  langid = {english},
  keywords = {AI deception,AI ethics,AI safety,Existential risk,Moral standing},
  file = {/Users/crathkopf/Zotero/storage/DUSLGPFS/Skretta - 2025 - AI deception and moral standing.pdf}
}

@misc{zou2025Representation,
  title = {Representation {{Engineering}}: {{A Top-Down Approach}} to {{AI Transparency}}},
  shorttitle = {Representation {{Engineering}}},
  author = {Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and Goel, Shashwat and Li, Nathaniel and Byun, Michael J. and Wang, Zifan and Mallen, Alex and Basart, Steven and Koyejo, Sanmi and Song, Dawn and Fredrikson, Matt and Kolter, J. Zico and Hendrycks, Dan},
  year = 2025,
  month = mar,
  number = {arXiv:2310.01405},
  eprint = {2310.01405},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.01405},
  urldate = {2026-01-21},
  abstract = {In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/crathkopf/Zotero/storage/JPSV6SQZ/Zou et al. - 2025 - Representation Engineering A Top-Down Approach to AI Transparency.pdf}
}
