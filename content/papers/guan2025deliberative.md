---
title: "Deliberative Alignment: Reasoning Enables Safer Language Models"
date: 2025-01-01
draft: false

# Paper metadata
authors: ["Guan, Melody Y.", "Joglekar, Manas", "Wallace, Eric", "Jain, Saachi", "Barak, Boaz", "Helyar, Alec", "Dias, Rachel", "Vallone, Andrea", "Ren, Hongyu", "Wei, Jason", "Chung, Hyung Won", "Toyer, Sam", "Heidecke, Johannes", "Beutel, Alex", "Glaese, Amelia"]
publication: "arXiv"
publication_year: 2025
doi: "10.48550/arXiv.2412.16339"
arxiv: "2412.16339"
paper_url: "https://doi.org/10.48550/arXiv.2412.16339"

# Summary
summary: "As large-scale language models increasingly impact safety-critical domains, ensuring their reliable adherence to well-defined principles remains a fundamental challenge. We introduce Deliberative Alignment, a new paradigm that directly teaches the model safety specifications and trains it to explicitly recall and accurately reason over the specifications before answering."
---

## Philosophically-flavored questions

*Analysis by Charles Rathkopf*
*Last updated: January 2026*

[Questions to be written]

---

## Abstract

As large-scale language models increasingly impact safety-critical domains, ensuring their reliable adherence to well-defined principles remains a fundamental challenge. We introduce Deliberative Alignment, a new paradigm that directly teaches the model safety specifications and trains it to explicitly recall and accurately reason over the specifications before answering. We used this approach to align OpenAI's o-series models, and achieved highly precise adherence to OpenAI's safety policies, without requiring human-written chain-of-thoughts or answers. Deliberative Alignment pushes the Pareto frontier by simultaneously increasing robustness to jailbreaks while decreasing overrefusal rates, and also improves out-of-distribution generalization. We demonstrate that reasoning over explicitly specified policies enables more scalable, trustworthy, and interpretable alignment.

---

### Citation for this analysis

Charles Rathkopf, "Philosophical Questions in Deliberative Alignment: Reasoning Enables Safer Language Models," *AI Deception Papers*, January 2026, https://doi.org/10.48550/arXiv.2412.16339
