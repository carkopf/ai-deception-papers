---
title: "Re-Evaluating Theory of Mind Evaluation in Large Language Models"
date: 2025-01-01
draft: false

# Paper metadata
authors: ["Hu, Jennifer", "Sosa, Felix", "Ullman, Tomer"]
publication: "arXiv"
publication_year: 2025
doi: "10.48550/arXiv.2502.21098"
arxiv: "2502.21098"
paper_url: "https://doi.org/10.48550/arXiv.2502.21098"

# Summary
summary: "The question of whether large language models (LLMs) possess Theory of Mind (ToM) -- often defined as the ability to reason about others' mental states -- has sparked significant scientific and public interest. However, the evidence as to whether LLMs possess ToM is mixed, and the recent growth in evaluations has not resulted in a convergence."
---

## Philosophically-flavored questions

*Analysis by Charles Rathkopf*
*Last updated: January 2026*

[Questions to be written]

---

## Abstract

The question of whether large language models (LLMs) possess Theory of Mind (ToM) -- often defined as the ability to reason about others' mental states -- has sparked significant scientific and public interest. However, the evidence as to whether LLMs possess ToM is mixed, and the recent growth in evaluations has not resulted in a convergence. Here, we take inspiration from cognitive science to re-evaluate the state of ToM evaluation in LLMs. We argue that a major reason for the disagreement on whether LLMs have ToM is a lack of clarity on whether models should be expected to match human behaviors, or the computations underlying those behaviors. We also highlight ways in which current evaluations may be deviating from "pure" measurements of ToM abilities, which also contributes to the confusion. We conclude by discussing several directions for future research, including the relationship between ToM and pragmatic communication, which could advance our understanding of artificial systems as well as human cognition.

---

### Citation for this analysis

Charles Rathkopf, "Philosophical Questions in Re-Evaluating Theory of Mind Evaluation in Large Language Models," *AI Deception Papers*, January 2026, https://doi.org/10.48550/arXiv.2502.21098
