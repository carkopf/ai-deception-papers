---
title: "AI Deception: A Survey of Examples, Risks, and Potential Solutions"
date: 2023-01-01
draft: false

# Paper metadata
authors: ["Park, Peter S.", "Goldstein, Simon", "O'Gara, Aidan", "Chen, Michael", "Hendrycks, Dan"]
publication: "arXiv"
publication_year: 2023
doi: "10.48550/arXiv.2308.14752"
arxiv: "2308.14752"
paper_url: "https://doi.org/10.48550/arXiv.2308.14752"

# Summary
summary: "This paper argues that a range of current AI systems have learned how to deceive humans. We define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth."
---

## Philosophically-flavored questions

*Analysis by Charles Rathkopf*
*Last updated: January 2026*

[Questions to be written]

---

## Abstract

This paper argues that a range of current AI systems have learned how to deceive humans. We define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth. We first survey empirical examples of AI deception, discussing both special-use AI systems (including Meta's CICERO) built for specific competitive situations, and general-purpose AI systems (such as large language models). Next, we detail several risks from AI deception, such as fraud, election tampering, and losing control of AI systems. Finally, we outline several potential solutions to the problems posed by AI deception: first, regulatory frameworks should subject AI systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect AI deception and to make AI systems less deceptive. Policymakers, researchers, and the broader public should work proactively to prevent AI deception from destabilizing the shared foundations of our society.

---

### Citation for this analysis

Charles Rathkopf, "Philosophical Questions in AI Deception: A Survey of Examples, Risks, and Potential Solutions," *AI Deception Papers*, January 2026, https://doi.org/10.48550/arXiv.2308.14752
