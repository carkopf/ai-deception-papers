---
title: "AI Deception and Moral Standing"
date: 2025-01-01
draft: false

# Paper metadata
authors: ["Skretta, Anton"]
publication: "Philosophical Studies"
publication_year: 2025
doi: "10.1007/s11098-025-02465-y"
arxiv: ""
paper_url: "https://doi.org/10.1007/s11098-025-02465-y"

# Summary
summary: "There is a tension between the presumptive moral standing of future artificial intelligence [AI] and the presently popular ways of thinking about certain AI safety measures. I focus here primarily on those safety measures aimed at mitigating risks associated with AI deception."
---

## Philosophically-flavored questions

*Analysis by Charles Rathkopf*
*Last updated: January 2026*

[Questions to be written]

---

## Abstract

There is a tension between the presumptive moral standing of future artificial intelligence [AI] and the presently popular ways of thinking about certain AI safety measures. I focus here primarily on those safety measures aimed at mitigating risks associated with AI deception. Some of the most serious risks of AI deception are those connected to robust deceptive capabilities. Most of the discussion of the risks posed by deception in the AI safety literature focuses on catastrophic risks posed by advanced, power-seeking AI systems. Robust deception plays a central role in many worries about our ability to control future AI. According to many, a superintelligent, power-seeking AI would be capable of and incentivized to rely on robust deception. But given all that a capacity for robust deception requires, any being capable of robust deception holds presumptive moral standing. In spite of our own legitimate interests, some safety measures may be ruled out by moral concern for AI themselves.

---

### Citation for this analysis

Charles Rathkopf, "Philosophical Questions in AI Deception and Moral Standing," *AI Deception Papers*, January 2026, https://doi.org/10.1007/s11098-025-02465-y
