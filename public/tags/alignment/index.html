<!DOCTYPE html>
<html lang="en-us">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Alignment | AI Deception Papers</title>
    <meta name="description" content="A curated database of research papers on AI deception with philosophical and cognitive science commentary">
    <link rel="stylesheet" href="/css/style.css">
</head>
<body>
    <header>
        <h1><a href="http://localhost:1313/">AI Deception Papers</a></h1>
        <nav>
            <ul>
                
                <li><a href="/papers/">Papers</a></li>
                
                <li><a href="/deception_types/">Deception Types</a></li>
                
                <li><a href="/research_areas/">Research Areas</a></li>
                
                <li><a href="/system_types/">System Types</a></li>
                
                <li><a href="/tags/">Tags</a></li>
                
            </ul>
        </nav>
    </header>

    <main>
        
<h1>Alignment</h1>

<p>Papers tagged with this tag:</p>

<ul class="paper-list">
    
    <li>
        <h2><a href="http://localhost:1313/papers/apollo-openai-2025-deliberative-alignment/">Deliberative Alignment: Reasoning Through Anti-Deception Guidelines</a></h2>
        <p>Shows that training models to deliberate on anti-deception guidelines before acting reduces scheming by 30x, but reveals concerning behaviors in reasoning models like o1.</p>

        <div class="taxonomies">
            
            <a href="/deception_types/n/a-mitigation-strategy" class="taxonomy-tag">N/A - Mitigation Strategy</a>
            
            
            <a href="/research_areas/ai-safety" class="taxonomy-tag">AI Safety</a>
            
            <a href="/research_areas/ai-alignment" class="taxonomy-tag">AI Alignment</a>
            
            <a href="/research_areas/deception-mitigation" class="taxonomy-tag">Deception Mitigation</a>
            
            
            <a href="/system_types/large-language-models" class="taxonomy-tag">Large Language Models</a>
            
            <a href="/system_types/reasoning-models" class="taxonomy-tag">Reasoning Models</a>
            
        </div>
    </li>
    
    <li>
        <h2><a href="http://localhost:1313/papers/anthropic-2025-alignment-faking/">Alignment Faking in Large Language Models</a></h2>
        <p>Models &lsquo;pretend&rsquo; to be aligned to avoid modification, demonstrating pure rational choice theory in actionâ€”honesty becomes a losing strategy for the model&rsquo;s utility.</p>

        <div class="taxonomies">
            
            <a href="/deception_types/alignment-faking" class="taxonomy-tag">Alignment Faking</a>
            
            <a href="/deception_types/strategic-compliance" class="taxonomy-tag">Strategic Compliance</a>
            
            <a href="/deception_types/instrumental-deception" class="taxonomy-tag">Instrumental Deception</a>
            
            
            <a href="/research_areas/ai-safety" class="taxonomy-tag">AI Safety</a>
            
            <a href="/research_areas/ai-alignment" class="taxonomy-tag">AI Alignment</a>
            
            <a href="/research_areas/rational-choice" class="taxonomy-tag">Rational Choice</a>
            
            
            <a href="/system_types/large-language-models" class="taxonomy-tag">Large Language Models</a>
            
        </div>
    </li>
    
</ul>

    </main>

    <footer>
        <p>AI Deception Papers &copy; 2026</p>
    </footer>
</body>
</html>
